<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Prediction</title>

<script src="site_libs/header-attrs-2.18/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">BST260 Final Project</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Exploratory Data Analysis</a>
</li>
<li>
  <a href="prediction.html">Prediction</a>
</li>
<li>
  <a href="Report.html">Report</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Prediction</h1>

</div>


<pre class="r"><code>library(tidyverse) 
library(dplyr)
library(dslabs)
library(ggplot2)
library(lubridate) 
library(caret)
library(HistData)
library(Lahman)
library(purrr)
library(tidyr)</code></pre>
<pre class="r"><code>d &lt;- read_csv(&#39;./data/train.csv&#39;)
d &lt;- d[, !(names(d) %in% c(&quot;surveyid&quot;, &quot;village&quot;, &quot;survey_date&quot;))]

na_cols &lt;- names(which(colSums(is.na(d))&gt;0))
d &lt;- d[ , !(names(d) %in% na_cols)]

# d$Class&lt;-as.factor(d$depressed) # convert class to factor
# levels(d$Class) &lt;- c(&#39;not_depressed&#39;, &#39;depressed&#39;) # names of factors
# summary(d$Class)

# Correct imbalance in data
#ROSE algorithm
library(ROSE)
drose &lt;- ROSE(depressed ~ ., N = 1143, data = d, seed = 260)$data
table(drose$depressed)</code></pre>
<pre><code>## 
##   0   1 
## 534 609</code></pre>
<pre class="r"><code>#Data partition
set.seed(260)
test_index &lt;- createDataPartition(d$depressed, times = 1, p = 0.2, list = FALSE)
test_set &lt;- d[test_index, ]
train_set &lt;- d[-test_index, ]

#predictor / response definition
predictor_variables &lt;- d[,-48]
response_variable &lt;- d$depressed
#swap to have minority class coded as 1
levels(response_variable) &lt;- c(&#39;0&#39;, &#39;1&#39;) 
table(d$depressed)</code></pre>
<pre><code>## 
##   0   1 
## 950 193</code></pre>
<div id="logistic-regression-with-tuned-parameter"
class="section level3">
<h3>Logistic Regression with tuned parameter</h3>
<pre class="r"><code>glm_train &lt;- glm(depressed ~ ., data = train_set, family = &quot;binomial&quot;)
summary(glm_train)</code></pre>
<pre><code>## 
## Call:
## glm(formula = depressed ~ ., family = &quot;binomial&quot;, data = train_set)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6634  -0.6411  -0.5339  -0.3522   2.5294  
## 
## Coefficients: (2 not defined because of singularities)
##                          Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)            -8.455e-01  6.845e-01  -1.235  0.21673   
## femaleres              -1.741e-02  3.534e-01  -0.049  0.96070   
## age                     5.845e-03  8.298e-03   0.704  0.48117   
## married                -3.380e-01  2.629e-01  -1.286  0.19848   
## children                3.652e-02  1.603e-01   0.228  0.81977   
## hhsize                  7.128e-03  1.347e-01   0.053  0.95781   
## edu                    -7.592e-02  3.634e-02  -2.089  0.03669 * 
## hh_children             3.761e-02  8.980e-02   0.419  0.67537   
## cons_nondurable         1.376e-02  1.299e-02   1.059  0.28943   
## asset_livestock         1.305e-03  3.610e-03   0.362  0.71765   
## asset_durable          -7.700e-04  4.242e-03  -0.182  0.85596   
## asset_phone             4.752e-03  4.785e-03   0.993  0.32065   
## asset_savings          -5.852e+04  4.158e+04  -1.407  0.15933   
## asset_land_owned_total  3.077e-02  6.891e-02   0.447  0.65518   
## asset_niceroof         -1.092e+01  8.828e+02  -0.012  0.99013   
## cons_allfood           -1.461e-02  1.351e-02  -1.082  0.27939   
## cons_ownfood           -4.990e-03  1.592e-02  -0.313  0.75401   
## cons_med_total         -4.014e-03  1.514e-02  -0.265  0.79092   
## cons_ed                -5.852e+04  4.158e+04  -1.407  0.15933   
## cons_social            -1.138e-02  2.177e-02  -0.523  0.60096   
## cons_other             -1.128e-02  1.411e-02  -0.800  0.42394   
## ent_wagelabor           1.189e-01  5.856e-01   0.203  0.83916   
## ent_ownfarm            -6.059e-01  3.145e-01  -1.927  0.05400 . 
## ent_business           -4.201e-01  4.240e-01  -0.991  0.32186   
## ent_nonagbusiness       3.415e-02  2.942e-01   0.116  0.90759   
## ent_employees          -4.400e-01  7.189e-01  -0.612  0.54048   
## ent_nonag_revenue       2.464e-03  1.313e-03   1.877  0.06058 . 
## ent_nonag_flowcost      8.070e-02  3.242e-02   2.489  0.01281 * 
## ent_farmrevenue        -3.388e-02  2.501e-02  -1.355  0.17554   
## ent_farmexpenses        1.794e-01  5.590e-02   3.210  0.00133 **
## ent_animalstockrev      1.115e-02  1.218e-02   0.916  0.35989   
## ent_total_cost         -5.852e+04  4.158e+04  -1.407  0.15933   
## fs_adskipm_often       -1.921e-02  1.754e-02  -1.095  0.27334   
## fs_adwholed_often       1.090e-01  3.355e-02   3.247  0.00117 **
## med_vacc_newborns              NA         NA      NA       NA   
## med_child_check                NA         NA      NA       NA   
## labor_primary          -2.661e-01  5.883e-01  -0.452  0.65105   
## wage_expenditures      -4.160e-04  3.269e-02  -0.013  0.98985   
## durable_investment     -1.345e-04  3.543e-03  -0.038  0.96970   
## nondurable_investment   5.852e+04  4.158e+04   1.407  0.15933   
## given_mpesa            -1.738e-01  9.240e-01  -0.188  0.85076   
## amount_given_mpesa      3.685e+05  9.687e+05   0.380  0.70368   
## received_mpesa          3.938e-01  4.254e-01   0.926  0.35458   
## amount_received_mpesa  -3.685e+05  9.687e+05  -0.380  0.70368   
## net_mpesa               3.685e+05  9.687e+05   0.380  0.70368   
## saved_mpesa            -1.758e-02  2.527e-01  -0.070  0.94455   
## amount_saved_mpesa     -3.852e-03  7.636e-03  -0.504  0.61399   
## early_survey            2.151e-01  3.168e-01   0.679  0.49708   
## day_of_week            -3.722e-02  5.570e-02  -0.668  0.50399   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 841.62  on 913  degrees of freedom
## Residual deviance: 780.71  on 867  degrees of freedom
## AIC: 874.71
## 
## Number of Fisher Scoring iterations: 13</code></pre>
<pre class="r"><code>pred &lt;- predict(glm_train, test_set, type=&quot;response&quot;)
pred &lt;- as.integer(pred&gt;0.18)
cm &lt;- confusionMatrix(as.factor(pred), as.factor(test_set$depressed))


library(pROC)
library(verification)
roc.curve(test_set$depressed, pred)</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre><code>## Area under the curve (AUC): 0.639</code></pre>
<pre class="r"><code>auc_log &lt;- as.numeric(auc(roc(test_set$depressed, pred)))


results &lt;- tibble(Method = &quot;Logistic Regression&quot;, AUC = auc_log,
                     F1 = cm$byClass[&quot;F1&quot;], Specificity = cm$byClass[&quot;Specificity&quot;], 
                     Balanced_Accuracy = cm$byClass[&quot;Balanced Accuracy&quot;])
results</code></pre>
<pre><code>## # A tibble: 1 × 5
##   Method                AUC    F1 Specificity Balanced_Accuracy
##   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;
## 1 Logistic Regression 0.639 0.792       0.571             0.639</code></pre>
</div>
<div id="classification-tree" class="section level3">
<h3>Classification Tree</h3>
<pre class="r"><code>library(rpart)
library(rpart.plot)
#build decision tree models on training set

tree.rose &lt;- rpart(depressed~ ., data = train_set, method = &#39;class&#39;)
rpart.plot(tree.rose)</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>printcp(tree.rose)</code></pre>
<pre><code>## 
## Classification tree:
## rpart(formula = depressed ~ ., data = train_set, method = &quot;class&quot;)
## 
## Variables actually used in tree construction:
## [1] cons_ed           cons_other        ent_farmexpenses  ent_ownfarm      
## [5] ent_total_cost    fs_adskipm_often  fs_adwholed_often
## 
## Root node error: 158/914 = 0.17287
## 
## n= 914 
## 
##         CP nsplit rel error xerror     xstd
## 1 0.010549      0   1.00000 1.0000 0.072353
## 2 0.010000      7   0.92405 1.1962 0.077494</code></pre>
<pre class="r"><code>pred.tree.rose &lt;- predict(tree.rose, newdata = test_set, type = &quot;prob&quot;)
pred.tree.rose &lt;- as.integer(pred.tree.rose[,2]&gt;0.2)
roc.curve(test_set$depressed, pred.tree.rose)</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<pre><code>## Area under the curve (AUC): 0.566</code></pre>
<pre class="r"><code>cm_tree &lt;- confusionMatrix(as.factor(pred.tree.rose), as.factor(test_set$depressed))
auc_tree &lt;- as.numeric(auc(roc(test_set$depressed, pred.tree.rose)))
result_tree &lt;- tibble(Method = &quot;Classification Tree&quot;, 
                                     AUC = auc_tree,
                                     F1 = cm_tree$byClass[&quot;F1&quot;], 
                                     Specificity = cm_tree$byClass[&quot;Specificity&quot;], 
                                     Balanced_Accuracy = cm_tree$byClass[&quot;Balanced Accuracy&quot;])
results &lt;- bind_rows(results, result_tree)</code></pre>
</div>
<div id="bagging-cart" class="section level3">
<h3>Bagging CART</h3>
<p>Bootstrapped Aggregation (Bagging) is an ensemble method that creates
multiple models of the same type from different sub-samples of the same
dataset. The predictions from each separate model are combined together
to provide a superior result. This approach has shown participially
effective for high-variance methods such as decision trees.</p>
<p>Here is bagging applied to the recursive partitioning decision tree
for our depression dataset.</p>
<pre class="r"><code>library(ipred)
fit_ipred &lt;- bagging(depressed~., data=train_set)
pred.ipred &lt;- predict(fit_ipred, newdata = test_set)
pred.ipred &lt;- as.integer(pred.ipred&gt;0.2)

roc.curve(test_set$depressed, pred.ipred)</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre><code>## Area under the curve (AUC): 0.667</code></pre>
<pre class="r"><code>cm_ipred &lt;- confusionMatrix(as.factor(pred.ipred), as.factor(test_set$depressed))

results &lt;- bind_rows(results, tibble(Method = &quot;Bagging&quot;, 
                      AUC = as.numeric(auc(roc(test_set$depressed, pred.ipred))),
                      F1 = cm_ipred$byClass[&quot;F1&quot;], Specificity = cm_ipred$byClass[&quot;Specificity&quot;],
                      Balanced_Accuracy = cm_ipred$byClass[&quot;Balanced Accuracy&quot;]))</code></pre>
</div>
<div id="knn" class="section level3">
<h3>kNN</h3>
<pre class="r"><code>control &lt;- trainControl(method = &quot;cv&quot;,number = 5, p = .8, classProbs = TRUE)
train_knn &lt;- train(train_set[, -which(names(train_set) == &quot;depressed&quot;)],
                   make.names(train_set$depressed), method = &quot;knn&quot;,
                   tuneGrid = data.frame(k = seq(3,10, 2)), trControl = control,
                   metric = &quot;ROC&quot;, maximize = TRUE
                   )
train_knn</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 914 samples
##  48 predictor
##   2 classes: &#39;X0&#39;, &#39;X1&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 732, 730, 731, 732, 731 
## Resampling results across tuning parameters:
## 
##   k  Accuracy   Kappa      
##   3  0.7867680  0.031216399
##   5  0.8064105  0.008498618
##   7  0.8227744  0.015016266
##   9  0.8271402  0.000000000
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 9.</code></pre>
<pre class="r"><code>fit_knn &lt;- knn3(train_set[, -which(names(train_set) == &quot;depressed&quot;)],
                factor(train_set$depressed),  k = train_knn$bestTune$k)
y_hat_knn &lt;- predict(fit_knn, test_set[, -which(names(train_set) == &quot;depressed&quot;)], type=&quot;class&quot;)

cm_knn &lt;- confusionMatrix(as.factor(y_hat_knn), as.factor(test_set$depressed))

roc.curve(test_set$depressed, y_hat_knn)</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre><code>## Area under the curve (AUC): 0.512</code></pre>
<pre class="r"><code>results &lt;- bind_rows(results, tibble(Method = &quot;kNN&quot;, AUC = as.numeric(auc(roc(as.numeric(test_set$depressed), as.numeric(y_hat_knn)))),
                      F1 = cm_knn$byClass[&quot;F1&quot;], Specificity = cm_knn$byClass[&quot;Specificity&quot;],
                      Balanced_Accuracy = cm_knn$byClass[&quot;Balanced Accuracy&quot;]))</code></pre>
</div>
<div id="random-forest" class="section level3">
<h3>Random Forest</h3>
<pre class="r"><code>control &lt;- trainControl(method = &quot;cv&quot;,number = 5, p = .8)
grid &lt;- data.frame(mtry=c(1, 5, sqrt(ncol(d))))
train_rf &lt;- train(train_set[, -which(names(train_set) == &quot;depressed&quot;)],
                   train_set$depressed, method = &quot;rf&quot;,
                   tuneGrid = grid, trControl = control,
                  metric = &quot;F1&quot;, maximize = TRUE)
train_rf$bestTune</code></pre>
<pre><code>##   mtry
## 3    7</code></pre>
<pre class="r"><code>library(randomForest)
fit_rf &lt;- randomForest(depressed~., data=train_set, mtry = train_rf$bestTune$mtry)
y_hat_rf &lt;- predict(fit_rf, test_set)
y_hat_rf &lt;- as.integer(y_hat_rf&gt;0.18)
roc.curve(test_set$depressed, y_hat_rf)</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre><code>## Area under the curve (AUC): 0.683</code></pre>
<pre class="r"><code>cm_rf &lt;- confusionMatrix(factor(y_hat_rf), factor(test_set$depressed))

results &lt;- bind_rows(results, tibble(Method = &quot;Random Forest&quot;, AUC = as.numeric(auc(roc(test_set$depressed, y_hat_rf))),
                      F1 = cm_rf$byClass[&quot;F1&quot;], Specificity = cm_rf$byClass[&quot;Specificity&quot;],
                      Balanced_Accuracy = cm_rf$byClass[&quot;Balanced Accuracy&quot;]))</code></pre>
</div>
<div id="gradient-boosted-machine" class="section level3">
<h3>Gradient Boosted Machine</h3>
<pre class="r"><code>library(gbm)

grid_gbm &lt;- expand.grid(n.trees = c(50, 100, 200), interaction.depth = c(1,2,3), 
                        shrinkage = c(0.1, 0.5, 0.8), n.minobsinnode = c(5, 10, 15))
train_gbm &lt;- train(depressed ~ ., data = train_set, method = &quot;gbm&quot;, trControl = control,
                   verbose = FALSE, tuneGrid = grid_gbm, metric = &quot;F1&quot;, maximize = TRUE)
train_gbm$bestTune</code></pre>
<pre><code>##    n.trees interaction.depth shrinkage n.minobsinnode
## 78     200                 3       0.8             10</code></pre>
<pre class="r"><code>fit_gbm &lt;- gbm(depressed ~., data=train_set, distribution = &quot;bernoulli&quot;)
y_hat_gbm &lt;- predict(fit_gbm, test_set)
y_hat_gbm &lt;- as.integer(y_hat_gbm&gt;-1.55)
roc.curve(test_set$depressed, y_hat_gbm)</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre><code>## Area under the curve (AUC): 0.694</code></pre>
<pre class="r"><code>cm_gbm &lt;- confusionMatrix(as.factor(y_hat_gbm), as.factor(test_set$depressed))

results &lt;- bind_rows(results, tibble(Method = &quot;Gradient Boosted Machine&quot;, AUC = as.numeric(auc(roc(test_set$depressed, y_hat_gbm))),
                      F1 = cm_gbm$byClass[&quot;F1&quot;], Specificity = cm_gbm$byClass[&quot;Specificity&quot;],
                      Balanced_Accuracy = cm_gbm$byClass[&quot;Balanced Accuracy&quot;]))</code></pre>
</div>
<div id="rose-data" class="section level2">
<h2>ROSE data</h2>
<p>From results before, Random Forest and Gradient Boosted Machine seem
to perform the best in terms of balanced accuracy and specificity
measure. Here we attempt these 2 models with resampled balanced
data.</p>
<pre class="r"><code>table(drose$depressed)</code></pre>
<pre><code>## 
##   0   1 
## 534 609</code></pre>
<pre class="r"><code>#Data partition
set.seed(260)
test_index &lt;- createDataPartition(drose$depressed, times = 1, p = 0.2, list = FALSE)
test_rose &lt;- d[test_index, ]
train_rose &lt;- d[-test_index, ] </code></pre>
<div id="random-forest-1" class="section level4">
<h4>Random Forest</h4>
<pre class="r"><code>control &lt;- trainControl(method = &quot;cv&quot;,number = 5, p = .8)
grid &lt;- data.frame(mtry=c(1, 5, sqrt(ncol(drose))))
train_rf_rose &lt;- train(train_rose[, -which(names(train_rose) == &quot;depressed&quot;)],
                  train_rose$depressed, method = &quot;rf&quot;, tuneGrid = grid, trControl = control,
                  metric = &quot;F1&quot;, maximize = TRUE)
train_rf_rose$bestTune</code></pre>
<pre><code>##   mtry
## 3    7</code></pre>
<pre class="r"><code>fit_rf_rose &lt;- randomForest(depressed~., data=train_rose, mtry = train_rf_rose$bestTune$mtry)
y_rose_rf &lt;- predict(fit_rf_rose, test_rose)
y_rose_rf &lt;- as.integer(y_hat_rf&gt;0.18)
roc.curve(test_rose$depressed, y_rose_rf)</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre><code>## Area under the curve (AUC): 0.683</code></pre>
<pre class="r"><code>cm_rf_rose &lt;- confusionMatrix(factor(y_rose_rf), factor(test_rose$depressed))

results &lt;- bind_rows(results, tibble(Method = &quot;ROSE - Random Forest&quot;, 
                       AUC = as.numeric(auc(roc(test_rose$depressed, y_rose_rf))),
                       F1 = cm_rf_rose$byClass[&quot;F1&quot;], 
                       Specificity = cm_rf_rose$byClass[&quot;Specificity&quot;],
                       Balanced_Accuracy = cm_rf_rose$byClass[&quot;Balanced Accuracy&quot;]))</code></pre>
</div>
<div id="gradient-boosted-machine-1" class="section level3">
<h3>Gradient Boosted Machine</h3>
<pre class="r"><code>library(gbm)
train_gbm_rose &lt;- train(depressed ~ ., data = train_rose, method = &quot;gbm&quot;, trControl = control,
                   verbose = FALSE, tuneGrid = grid_gbm, metric = &quot;F1&quot;, maximize = TRUE)
#train_gbm_rose$bestTune


fit_gbm_rose &lt;- gbm(depressed ~., data=train_rose, distribution = &quot;bernoulli&quot;)
y_rose_gbm &lt;- predict(fit_gbm_rose, test_rose)
y_rose_gbm &lt;- as.integer(y_rose_gbm&gt;-1.55)
roc.curve(test_rose$depressed, y_rose_gbm)</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre><code>## Area under the curve (AUC): 0.665</code></pre>
<pre class="r"><code>cm_gbm_rose &lt;- confusionMatrix(as.factor(y_rose_gbm), as.factor(test_rose$depressed))

results &lt;- bind_rows(results, tibble(Method = &quot;ROSE - Gradient Boosted Machine&quot;, 
                                     AUC = as.numeric(auc(roc(test_rose$depressed, y_rose_gbm))),
                                     F1 = cm_gbm_rose$byClass[&quot;F1&quot;], 
                                     Specificity = cm_gbm_rose$byClass[&quot;Specificity&quot;],
                                     Balanced_Accuracy = cm_gbm_rose$byClass[&quot;Balanced Accuracy&quot;]))</code></pre>
<pre class="r"><code>results</code></pre>
<pre><code>## # A tibble: 8 × 5
##   Method                            AUC    F1 Specificity Balanced_Accuracy
##   &lt;chr&gt;                           &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;
## 1 Logistic Regression             0.639 0.792      0.571              0.639
## 2 Classification Tree             0.566 0.898      0.2                0.566
## 3 Bagging                         0.667 0.829      0.571              0.667
## 4 kNN                             0.512 0.917      0.0286             0.512
## 5 Random Forest                   0.683 0.747      0.743              0.683
## 6 Gradient Boosted Machine        0.694 0.724      0.8                0.694
## 7 ROSE - Random Forest            0.683 0.747      0.743              0.683
## 8 ROSE - Gradient Boosted Machine 0.665 0.793      0.629              0.665</code></pre>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
