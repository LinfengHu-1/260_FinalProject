---
title: "Prediction"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse) 
library(dplyr)
library(dslabs)
library(ggplot2)
library(lubridate) 
library(caret)
library(HistData)
library(Lahman)
library(purrr)
library(tidyr)
```

```{r}
d <- read_csv('./data/train.csv')
d <- d[, !(names(d) %in% c("surveyid", "village", "survey_date"))]

na_cols <- names(which(colSums(is.na(d))>0))
d <- d[ , !(names(d) %in% na_cols)]

# d$Class<-as.factor(d$depressed) # convert class to factor
# levels(d$Class) <- c('not_depressed', 'depressed') # names of factors
# summary(d$Class)

# Correct imbalance in data
#ROSE algorithm
library(ROSE)
drose <- ROSE(depressed ~ ., N = 1143, data = d, seed = 260)$data
table(drose$depressed)

#Data partition
set.seed(260)
test_index <- createDataPartition(d$depressed, times = 1, p = 0.2, list = FALSE)
test_set <- d[test_index, ]
train_set <- d[-test_index, ]

#predictor / response definition
predictor_variables <- d[,-48]
response_variable <- d$depressed
#swap to have minority class coded as 1
levels(response_variable) <- c('0', '1') 
table(d$depressed)
```


### Logistic Regression with tuned parameter
```{r}
glm_train <- glm(depressed ~ ., data = train_set, family = "binomial")
summary(glm_train)
pred <- predict(glm_train, test_set, type="response")
pred <- as.integer(pred>0.18)
cm <- confusionMatrix(as.factor(pred), as.factor(test_set$depressed))


library(pROC)
library(verification)
roc.curve(test_set$depressed, pred)
auc_log <- as.numeric(auc(roc(test_set$depressed, pred)))


results <- tibble(Method = "Logistic Regression", AUC = auc_log,
                     F1 = cm$byClass["F1"], Specificity = cm$byClass["Specificity"], 
                     Balanced_Accuracy = cm$byClass["Balanced Accuracy"])
results
```


### Classification Tree
```{r}
library(rpart)
library(rpart.plot)
#build decision tree models on training set

tree.rose <- rpart(depressed~ ., data = train_set, method = 'class')
rpart.plot(tree.rose)
printcp(tree.rose)

pred.tree.rose <- predict(tree.rose, newdata = test_set, type = "prob")
pred.tree.rose <- as.integer(pred.tree.rose[,2]>0.2)
roc.curve(test_set$depressed, pred.tree.rose)

cm_tree <- confusionMatrix(as.factor(pred.tree.rose), as.factor(test_set$depressed))
auc_tree <- as.numeric(auc(roc(test_set$depressed, pred.tree.rose)))
result_tree <- tibble(Method = "Classification Tree", 
                                     AUC = auc_tree,
                                     F1 = cm_tree$byClass["F1"], 
                                     Specificity = cm_tree$byClass["Specificity"], 
                                     Balanced_Accuracy = cm_tree$byClass["Balanced Accuracy"])
results <- bind_rows(results, result_tree)
```

### Bagging CART

Bootstrapped Aggregation (Bagging) is an ensemble method that creates multiple models of the same type from different sub-samples of the same dataset. The predictions from each separate model are combined together to provide a superior result. This approach has shown participially effective for high-variance methods such as decision trees.

Here is bagging applied to the recursive partitioning decision tree for our depression dataset.
```{r}
library(ipred)
fit_ipred <- bagging(depressed~., data=train_set)
pred.ipred <- predict(fit_ipred, newdata = test_set)
pred.ipred <- as.integer(pred.ipred>0.2)

roc.curve(test_set$depressed, pred.ipred)
cm_ipred <- confusionMatrix(as.factor(pred.ipred), as.factor(test_set$depressed))

results <- bind_rows(results, tibble(Method = "Bagging", 
                      AUC = as.numeric(auc(roc(test_set$depressed, pred.ipred))),
                      F1 = cm_ipred$byClass["F1"], Specificity = cm_ipred$byClass["Specificity"],
                      Balanced_Accuracy = cm_ipred$byClass["Balanced Accuracy"]))
```


### kNN

```{r}
control <- trainControl(method = "cv",number = 5, p = .8, classProbs = TRUE)
train_knn <- train(train_set[, -which(names(train_set) == "depressed")],
                   make.names(train_set$depressed), method = "knn",
                   tuneGrid = data.frame(k = seq(3,10, 2)), trControl = control,
                   metric = "ROC", maximize = TRUE
                   )
train_knn
fit_knn <- knn3(train_set[, -which(names(train_set) == "depressed")],
                factor(train_set$depressed),  k = train_knn$bestTune$k)
y_hat_knn <- predict(fit_knn, test_set[, -which(names(train_set) == "depressed")], type="class")

cm_knn <- confusionMatrix(as.factor(y_hat_knn), as.factor(test_set$depressed))

roc.curve(test_set$depressed, y_hat_knn)

results <- bind_rows(results, tibble(Method = "kNN", AUC = as.numeric(auc(roc(as.numeric(test_set$depressed), as.numeric(y_hat_knn)))),
                      F1 = cm_knn$byClass["F1"], Specificity = cm_knn$byClass["Specificity"],
                      Balanced_Accuracy = cm_knn$byClass["Balanced Accuracy"]))
```


### Random Forest

```{r}
control <- trainControl(method = "cv",number = 5, p = .8)
grid <- data.frame(mtry=c(1, 5, sqrt(ncol(d))))
train_rf <- train(train_set[, -which(names(train_set) == "depressed")],
                   train_set$depressed, method = "rf",
                   tuneGrid = grid, trControl = control,
                  metric = "F1", maximize = TRUE)
train_rf$bestTune

library(randomForest)
fit_rf <- randomForest(depressed~., data=train_set, mtry = train_rf$bestTune$mtry)
y_hat_rf <- predict(fit_rf, test_set)
y_hat_rf <- as.integer(y_hat_rf>0.18)
roc.curve(test_set$depressed, y_hat_rf)
cm_rf <- confusionMatrix(factor(y_hat_rf), factor(test_set$depressed))

results <- bind_rows(results, tibble(Method = "Random Forest", AUC = as.numeric(auc(roc(test_set$depressed, y_hat_rf))),
                      F1 = cm_rf$byClass["F1"], Specificity = cm_rf$byClass["Specificity"],
                      Balanced_Accuracy = cm_rf$byClass["Balanced Accuracy"]))
```


### Gradient Boosted Machine

```{r}
library(gbm)

grid_gbm <- expand.grid(n.trees = c(50, 100, 200), interaction.depth = c(1,2,3), 
                        shrinkage = c(0.1, 0.5, 0.8), n.minobsinnode = c(5, 10, 15))
train_gbm <- train(depressed ~ ., data = train_set, method = "gbm", trControl = control,
                   verbose = FALSE, tuneGrid = grid_gbm, metric = "F1", maximize = TRUE)
train_gbm$bestTune


fit_gbm <- gbm(depressed ~., data=train_set, distribution = "bernoulli")
y_hat_gbm <- predict(fit_gbm, test_set)
y_hat_gbm <- as.integer(y_hat_gbm>-1.55)
roc.curve(test_set$depressed, y_hat_gbm)

cm_gbm <- confusionMatrix(as.factor(y_hat_gbm), as.factor(test_set$depressed))

results <- bind_rows(results, tibble(Method = "Gradient Boosted Machine", AUC = as.numeric(auc(roc(test_set$depressed, y_hat_gbm))),
                      F1 = cm_gbm$byClass["F1"], Specificity = cm_gbm$byClass["Specificity"],
                      Balanced_Accuracy = cm_gbm$byClass["Balanced Accuracy"]))
```


## ROSE data

From results before, Random Forest and Gradient Boosted Machine seem to perform the best in terms of balanced accuracy and specificity measure. Here we attempt these 2 models with resampled balanced data.

```{r}
table(drose$depressed)

#Data partition
set.seed(260)
test_index <- createDataPartition(drose$depressed, times = 1, p = 0.2, list = FALSE)
test_rose <- d[test_index, ]
train_rose <- d[-test_index, ] 
```

#### Random Forest
```{r}
control <- trainControl(method = "cv",number = 5, p = .8)
grid <- data.frame(mtry=c(1, 5, sqrt(ncol(drose))))
train_rf_rose <- train(train_rose[, -which(names(train_rose) == "depressed")],
                  train_rose$depressed, method = "rf", tuneGrid = grid, trControl = control,
                  metric = "F1", maximize = TRUE)
train_rf_rose$bestTune


fit_rf_rose <- randomForest(depressed~., data=train_rose, mtry = train_rf_rose$bestTune$mtry)
y_rose_rf <- predict(fit_rf_rose, test_rose)
y_rose_rf <- as.integer(y_hat_rf>0.18)
roc.curve(test_rose$depressed, y_rose_rf)
cm_rf_rose <- confusionMatrix(factor(y_rose_rf), factor(test_rose$depressed))

results <- bind_rows(results, tibble(Method = "ROSE - Random Forest", 
                       AUC = as.numeric(auc(roc(test_rose$depressed, y_rose_rf))),
                       F1 = cm_rf_rose$byClass["F1"], 
                       Specificity = cm_rf_rose$byClass["Specificity"],
                       Balanced_Accuracy = cm_rf_rose$byClass["Balanced Accuracy"]))
```


### Gradient Boosted Machine
```{r}
library(gbm)
train_gbm_rose <- train(depressed ~ ., data = train_rose, method = "gbm", trControl = control,
                   verbose = FALSE, tuneGrid = grid_gbm, metric = "F1", maximize = TRUE)
#train_gbm_rose$bestTune


fit_gbm_rose <- gbm(depressed ~., data=train_rose, distribution = "bernoulli")
y_rose_gbm <- predict(fit_gbm_rose, test_rose)
y_rose_gbm <- as.integer(y_rose_gbm>-1.55)
roc.curve(test_rose$depressed, y_rose_gbm)

cm_gbm_rose <- confusionMatrix(as.factor(y_rose_gbm), as.factor(test_rose$depressed))

results <- bind_rows(results, tibble(Method = "ROSE - Gradient Boosted Machine", 
                                     AUC = as.numeric(auc(roc(test_rose$depressed, y_rose_gbm))),
                                     F1 = cm_gbm_rose$byClass["F1"], 
                                     Specificity = cm_gbm_rose$byClass["Specificity"],
                                     Balanced_Accuracy = cm_gbm_rose$byClass["Balanced Accuracy"]))
```

```{r}
results
```



